// The generated lexer module will start with this code
{
module book.Lexer
open FSharp.Text.Lexing
open System
// open the module that defines the tokens
open Parser
// Set the language to English such that 4.0 is parsed as 4 and not 40.
System.Globalization.CultureInfo.CurrentCulture <- new System.Globalization.CultureInfo("en-US")
}

// We define macros for some regular expressions we will use later
let num         = ['0'-'9']+ 
let word        = ['a'-'z''A'-'Z'] ['a'-'z''A'-'Z''0'-'9''_']*
let whitespace  = [' ' '\t']
let newline     = "\n\r" | '\n' | '\r'

// We define now the rules for recognising and building tokens
// for each of the tokens of our language we need a rule
// NOTE: rules are applied in order top-down.
//       This is important when tokens overlap (not in this example)
rule tokenize = parse
// deal with tokens that need to be ignored (skip them)
| whitespace    { tokenize lexbuf }
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }
// deal with tokens that need to be built
| num           { NUM(int (LexBuffer<_>.LexemeString lexbuf)) }
| "->"          { THEN  }
| ":="          { ASS   }
| '*'           { TIMES }
| '/'           { DIV   }
| '+'           { PLUS  }
| '-'           { MINUS }
| '^'           { POW   }
| '('           { LPAR  }
| ')'           { RPAR  }
| eof           { EOF   }
| '&'           { ANDBIT }
| '|'           { ORBIT  }
| "&&"          { AND   }
| "||"          { OR    }
| '='           { EQ    }
| '!'           { BANG  }
| '['           { LSQB  }
| ']'           { RSQB  }
| "[]"          { ELSE  }
| '<'           { LS    }
| '>'           { GR    }
| ';'           { SEMI  }
| "if"          { IF    }
| "fi"          { FI    }
| "do"          { DO    } 
| "od"          { OD    }
| "skip"        { SKIP  }
| "true"        { TRUE  }
| "false"       { FALSE }
| word          { WORD(LexBuffer<_>.LexemeString lexbuf) }


